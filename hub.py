# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'hub.ui'
#
# Created by: PyQt5 UI code generator 5.15.4
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.


from PyQt5 import QtCore, QtGui, QtWidgets

import argparse
import os
import sys
from pathlib import Path
import cv2

import torch

from PyQt5.QtGui import *
from PyQt5.QtWidgets import *
from PyQt5.QtCore import *

import os
import shutil

import time
import glob

import text2emotion as te
from textblob import TextBlob
import cohere
co = cohere.Client('cohere API key here')


# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'se1.ui'
#
# Created by: PyQt5 UI code generator 5.15.6
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.
import os
import pandas as pd
import tensorflow as tf
import numpy as np
import pickle


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding
from tensorflow.keras.layers import TextVectorization

from PIL import Image
#from clip_interrogator import Interrogator, Config
#@title Setup
import os, subprocess
#ci = Interrogator(Config(clip_model_name="ViT-B-32/openai"))
#print(ci.interrogate(image))

import sys
sys.path.append('src/blip')
sys.path.append('clip-interrogator')

from clip_interrogator import Config, Interrogator

# download cache files
"""
print("Download preprocessed cache files...")
CACHE_URLS = [
    #'https://huggingface.co/pharma/ci-preprocess/raw/main/ViT-L-14_openai_artists.pkl',
    #'https://huggingface.co/pharma/ci-preprocess/raw/main/ViT-L-14_openai_flavors.pkl',
    #'https://huggingface.co/pharma/ci-preprocess/raw/main/ViT-L-14_openai_mediums.pkl',
    #'https://huggingface.co/pharma/ci-preprocess/raw/main/ViT-L-14_openai_movements.pkl',
    #'https://huggingface.co/pharma/ci-preprocess/raw/main/ViT-L-14_openai_trendings.pkl',
    #'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_artists.pkl',
    #'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_flavors.pkl',
    #'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_mediums.pkl',
    #'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_movements.pkl',
    #'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_trendings.pkl',
]
os.makedirs('cache', exist_ok=True)
for url in CACHE_URLS:
    print(subprocess.run(['wget', url, '-P', 'cache'], stdout=subprocess.PIPE).stdout.decode('utf-8'))
"""
config = Config()
config.blip_num_beams = 64
config.blip_offload = False
config.chunk_size = 2048
config.flavor_intermediate_count = 2048

ci = Interrogator(config)

def inference(image, mode, clip_model_name, best_max_flavors=32):
    if clip_model_name != ci.config.clip_model_name:
        ci.config.clip_model_name = clip_model_name
        ci.load_clip_model()
    image = image.convert('RGB')
    if mode == 'best':
        return ci.interrogate(image, max_flavors=int(best_max_flavors))
    elif mode == 'classic':
        return ci.interrogate_classic(image)
    else:
        return ci.interrogate_fast(image)

from PIL import Image
#from clip_interrogator import Interrogator, Config

#ci = Interrogator(Config(clip_model_name="ViT-B-32/openai"))
#print(ci.interrogate(image))

import sys
sys.path.append('src/blip')
sys.path.append('clip-interrogator')

from clip_interrogator import Config, Interrogator

config = Config()
config.blip_num_beams = 64
config.blip_offload = False
config.chunk_size = 2048
config.flavor_intermediate_count = 2048

ci = Interrogator(config)

def inference(image, mode, clip_model_name, best_max_flavors=16):
    if clip_model_name != ci.config.clip_model_name:
        ci.config.clip_model_name = clip_model_name
        ci.load_clip_model()
    image = image.convert('RGB')
    if mode == 'best':
        return ci.interrogate(image, max_flavors=int(best_max_flavors))
    elif mode == 'classic':
        return ci.interrogate_classic(image)
    else:
        return ci.interrogate_fast(image)
        
df = pd.read_csv(os.path.join('jigsaw-toxic-comment-classification-challenge', 'train.csv', 'all2.csv'), encoding='latin-1')
df.head()

from tensorflow.keras.layers import TextVectorization

X = df['comment_text'].astype(str)#.apply(pd.to_numeric)
y = df[df.columns[2:]].values.astype(int)
#y /= y.max()
#X_train["x3"] = X_train["x3"]
#tf.convert_to_tensor(X, dtype=tf.float32)
print(X.dtypes)

MAX_FEATURES = 200000 # number of words in the vocab

vectorizer = TextVectorization(max_tokens=MAX_FEATURES,
                               output_sequence_length=1800,
                               output_mode='int')
							
vectorizer.adapt(X.values)
vectorized_text = vectorizer(X.values)
                               
model = Sequential()
# Create the embedding layer 
numclasses = 6
model.add(Embedding(MAX_FEATURES+1, 32))
# Bidirectional LSTM Layer
model.add(Bidirectional(LSTM(32, activation='tanh')))
# Feature extractor Fully connected layers
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
# Final layer 
model.add(Dense(6, activation='sigmoid'))

model.compile(loss='BinaryCrossentropy', optimizer='Adam')

model.summary()
#filename = 'model/afraid2.h5'
history = None
"""
try:
    #history = pickle.load(open(filename, 'rb'))
    #result = loaded_model.score(X_test, Y_test)
    print("Loading model")
    history = tf.keras.models.load_model(filename)
    #result = history.score(X_test, Y_test)
    print("Loaded Model")
except Exception as e:
    print(e)
    history = model.fit(train, epochs=10, validation_data=val)
    #pickle.dump(history, open(filename, 'wb'))
    model.save(filename)
"""
"""
translated = "when I saw an anime girl surrounded by balloons, magical colored theme, kawaii decora rainbowcore, magical fairy background, cat theme banner, official character illustration, sparkling magical girl, decora inspired illustrations, airy theme, screenshot from a 2012s anime, clematis theme banner, decora inspired, witchlight carnival, my dress up darling anime, official artwork I felt emotions such as beauty, astonishment, mesmerization, admiration, enchantment, amazement, admiration, happiness, excitement, peace, serenity, amazement, calmness, gratification, coolness, awesomeness, freshness,"
input_text = vectorizer(translated)
print(translated)
df.columns[2:]

res = history.predict(np.expand_dims(input_text, 0))*100
print(res)
"""
class ScaledPixmapLabel(QtWidgets.QLabel):
    def paintEvent(self, event):
        if self.pixmap():
            pm = self.pixmap()
            originalRatio = pm.width() / pm.height()
            currentRatio = self.width() / self.height()
            if originalRatio != currentRatio:
                qp = QtGui.QPainter(self)
                pm = self.pixmap().scaled(self.size(), QtCore.Qt.KeepAspectRatio, QtCore.Qt.SmoothTransformation)
                rect = QtCore.QRect(0, 0, pm.width(), pm.height())
                rect.moveCenter(self.rect().center())
                qp.drawPixmap(rect, pm)
                return
        super().paintEvent(event)

class Ui_MainWindow(object):
    loaded_model = None
    cam_running = None
    vid_running = None
    opt = None
    
    num_stop = 1 
    output_folder = 'output/'
    vid_writer = None
    
    openfile_name_model = None
    vid_name = None
    img_name = None
    cap_statue = None
    save_dir = None
    img_over = None

    timer = QtCore.QTimer()
    
    #        Button_open_cam.clicked.connect(video_button)
    cap_video = 0
    flag = 0
    img = []
    
    video_stream = None
    
    alreadystarted_cam = None
    alreadystarted_vid = None
    cam_port = 0
    cam = cv2.VideoCapture(cam_port)
    
    curr_framecount = 0
    curr_fps = 30
    curr_maxframe = 0
    
    recording = None
    saving = None
    
    thistime = None
    
    selected = None
    
    def init_slots(self):
        self.button_load_model.clicked.connect(self.load_model)

        self.button_load_image.clicked.connect(self.open_img)
        self.button_load_video.clicked.connect(self.open_vid)
        self.button_load_camera.clicked.connect(self.open_cam)
        self.button_record.clicked.connect(self.record)
        self.button_save.clicked.connect(self.save)
        # self.ui.pushButton_9.clicked.connect(self.save_ss)
        # self.timer_video.timeout.connect(self.show_video_cam_frame)

        self.button_load_image.setDisabled(True)
        self.button_load_video.setDisabled(True)
        self.button_load_camera.setDisabled(True)
        #self.button_record.setDisabled(True)
        #self.button_save.setDisabled(True)
        
        pass
    def open_img(self):
        self.selected = "img"
        self.button_record.setDisabled(True)
        self.button_save.setDisabled(True)
        # try except
        self.cam_running = False
        self.vid_running = False
        dir = 'detections/images'
        if not os.path.exists(dir):
            #shutil.rmtree(dir)
            os.makedirs(dir)
        try:
            # self.img_name 选择图片路径
            self.img_name, _ = QtWidgets.QFileDialog.getOpenFileName(None, "Select Image", "data/images", "*.jpg ; *.png ; All Files(*)")
        except OSError as reason:
            print(str(reason))
        else:
            try:
                im0 = cv2.imread(self.img_name)
                im0 = cv2.cvtColor(im0, cv2.COLOR_BGR2RGB)
                FlippedImage = im0
                image = QtGui.QImage(FlippedImage, FlippedImage.shape[1],FlippedImage.shape[0], FlippedImage.shape[1] * 3, QtGui.QImage.Format_RGB888)
                pix = QtGui.QPixmap(image)
                
                #ConvertToQtFormat = QImage(FlippedImage.data, FlippedImage.shape[1], FlippedImage.shape[0], QImage.Format_RGB888)
                #Pic = ConvertToQtFormat.scaled(640, 640, Qt.KeepAspectRatio)
                #cv2.imshow("test", FlippedImage)
                img = Image.open(self.img_name).convert('RGB')
                self.image_box_1.setPixmap(QtGui.QPixmap(pix))
                
                translated = inference(img, "fast", clip_model_name="ViT-B-32/openai")
                print(translated)
                res = None
                try:
                    mainprompttemplate = 'When I saw ' + translated + ', I felt emotions such as'

                    response = co.generate(
                      prompt= mainprompttemplate
                    )
                    somestr = '{}'.format(response.generations[0].text)
                    input_text = mainprompttemplate + somestr
                    print(input_text)
                    translated = input_text
                    input_text = vectorizer(input_text)
                    
                    res = self.loaded_model.predict(np.expand_dims(input_text, 0))
                    #print(res, res[0][0])
                except Exception as e:
                    input_text = vectorizer(translated)
                    res = self.loaded_model.predict(np.expand_dims(input_text, 0))
                    #print(res, res[0][0])
                
                im0 = cv2.imread("emotions.jpg")
                overlay = im0.copy()
                
                emotion = te.get_emotion(translated)
                
                thismodel = tf.keras.models.load_model("model/happy2.h5")
                res0 = thismodel.predict(np.expand_dims(input_text, 0))
                print(res, res[0][0])
                print(res0)
                print(emotion)
                # A filled circle
                cv2.circle(overlay, (820, 300), int(65*2*emotion["Happy"]*res0[0][0]), (20,20,20), -1)
                cv2.circle(overlay, (820, 430), int(65*2*emotion["Happy"]*res0[0][1]), (20,20,20), -1)
                cv2.circle(overlay, (820, 570), int(65*2*emotion["Happy"]*res0[0][2]), (20,20,20), -1)
                cv2.circle(overlay, (820, 720), int(65*2*emotion["Happy"]*res0[0][3]), (20,20,20), -1)
                cv2.circle(overlay, (820, 870), int(65*2*emotion["Happy"]*res0[0][4]), (20,20,20), -1)
                
                thismodel = tf.keras.models.load_model("model/sad2.h5")
                res0 = thismodel.predict(np.expand_dims(input_text, 0))
                print(res0)
                cv2.circle(overlay, (820, 1700), int(65*2*emotion["Sad"]*res0[0][0]), (20,20,20), -1)
                cv2.circle(overlay, (820, 1550), int(65*2*emotion["Sad"]*res0[0][1]), (20,20,20), -1)
                cv2.circle(overlay, (820, 1420), int(65*2*emotion["Sad"]*res0[0][2]), (20,20,20), -1)
                cv2.circle(overlay, (820, 1275), int(65*2*emotion["Sad"]*res0[0][3]), (20,20,20), -1)
                cv2.circle(overlay, (820, 1135), int(65*2*emotion["Sad"]*res0[0][4]), (20,20,20), -1)
                
                thismodel = tf.keras.models.load_model("model/anxious2.h5")
                res0 = thismodel.predict(np.expand_dims(input_text, 0))
                print(res0)
                cv2.circle(overlay, (1430, 630), int(65*2*(emotion["Surprise"]*0.25+emotion["Fear"]*0.75)*res0[0][0]), (20,20,20), -1)
                cv2.circle(overlay, (1300, 710), int(65*2*(emotion["Surprise"]*0.25+emotion["Fear"]*0.75)*res0[0][1]), (20,20,20), -1)
                cv2.circle(overlay, (1170, 790), int(65*2*(emotion["Surprise"]*0.25+emotion["Fear"]*0.75)*res0[0][2]), (20,20,20), -1)
                cv2.circle(overlay, (1050, 850), int(65*2*(emotion["Surprise"]*0.25+emotion["Fear"]*0.75)*res0[0][3]), (20,20,20), -1)
                cv2.circle(overlay, (920, 940), int(65*2*(emotion["Surprise"]*0.25+emotion["Fear"]*0.75)*res0[0][4]), (20,20,20), -1)
                
                thismodel = tf.keras.models.load_model("model/angry2.h5")
                res0 = thismodel.predict(np.expand_dims(input_text, 0))
                print(res0)
                cv2.circle(overlay, (1430, 1330), int(65*2*emotion["Angry"]*res0[0][0]), (20,20,20), -1)
                cv2.circle(overlay, (1300, 1260), int(65*2*emotion["Angry"]*res0[0][1]), (20,20,20), -1)
                cv2.circle(overlay, (1170, 1200), int(65*2*emotion["Angry"]*res0[0][2]), (20,20,20), -1)
                cv2.circle(overlay, (1050, 1130), int(65*2*emotion["Angry"]*res0[0][3]), (20,20,20), -1)
                cv2.circle(overlay, (920, 1060), int(65*2*emotion["Angry"]*res0[0][4]), (20,20,20), -1)
                
                thismodel = tf.keras.models.load_model("model/afraid2.h5")
                res0 = thismodel.predict(np.expand_dims(input_text, 0))
                print(res0)
                cv2.circle(overlay, (220, 1370), int(65*2*emotion["Fear"]*res0[0][0]), (20,20,20), -1)
                cv2.circle(overlay, (330, 1300), int(65*2*emotion["Fear"]*res0[0][1]), (20,20,20), -1)
                cv2.circle(overlay, (450, 1220), int(65*2*emotion["Fear"]*res0[0][2]), (20,20,20), -1)
                cv2.circle(overlay, (560, 1150), int(65*2*emotion["Fear"]*res0[0][3]), (20,20,20), -1)
                cv2.circle(overlay, (685, 1070), int(65*2*emotion["Fear"]*res0[0][4]), (20,20,20), -1)
                
                thismodel = tf.keras.models.load_model("model/excited2.h5")
                res0 = thismodel.predict(np.expand_dims(input_text, 0))
                print(res0)
                cv2.circle(overlay, (220, 630), int(65*2*(emotion["Surprise"]*0.3+emotion["Happy"]*0.6)*res0[0][0]), (20,20,20), -1)
                cv2.circle(overlay, (330, 710), int(65*2*(emotion["Surprise"]*0.3+emotion["Happy"]*0.6)*res0[0][1]), (20,20,20), -1)
                cv2.circle(overlay, (450, 790), int(65*2*(emotion["Surprise"]*0.3+emotion["Happy"]*0.6)*res0[0][2]), (20,20,20), -1)
                cv2.circle(overlay, (560, 850), int(65*2*(emotion["Surprise"]*0.3+emotion["Happy"]*0.6)*res0[0][3]), (20,20,20), -1)
                cv2.circle(overlay, (685, 940), int(65*2*(emotion["Surprise"]*0.3+emotion["Happy"]*0.6)*res0[0][4]), (20,20,20), -1)
                
                tf.keras.backend.clear_session()
                alpha = 0.4  # Transparency factor.

                # Following line overlays transparent rectangle
                # over the image
                image_new = cv2.addWeighted(overlay, alpha, im0, 1 - alpha, 0)
                im0 = cv2.cvtColor(image_new, cv2.COLOR_BGR2RGB)
                FlippedImage = im0
                image = QtGui.QImage(FlippedImage, FlippedImage.shape[1],FlippedImage.shape[0], FlippedImage.shape[1] * 3, QtGui.QImage.Format_RGB888)
                pix = QtGui.QPixmap(image)
                self.image_box_2.setPixmap(QtGui.QPixmap(pix))
                """
                results.ims
                results.render()
                for im in results.ims:
                    #im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
                    #cv2.imshow("test", im)
                    FlippedImage = im
                    image = QtGui.QImage(FlippedImage, FlippedImage.shape[1],FlippedImage.shape[0], FlippedImage.shape[1] * 3, QtGui.QImage.Format_RGB888)
                    pix = QtGui.QPixmap(image)
                    #ConvertToQtFormat = QImage(FlippedImage.data, FlippedImage.shape[1], FlippedImage.shape[0], QImage.Format_RGB888)
                    #Pic = ConvertToQtFormat.scaled(640, 640, Qt.KeepAspectRatio)
                    #cv2.imshow("test", FlippedImage)
                    FlippedImage = cv2.cvtColor(FlippedImage, cv2.COLOR_BGR2RGB)
                    cv2.imwrite("detections/images/" + str(time.time()) + ".jpg", FlippedImage)
                    self.image_box_2.setPixmap(QtGui.QPixmap(pix))
                """
            except Exception as e:
                print(e)
        pass
    def open_vid(self):
        self.selected = "video"
        self.cam_running = False
        self.button_record.setDisabled(False)
        self.button_save.setDisabled(False)
        try:
            # self.img_name 选择图片路径
            self.vid_name, _ = QtWidgets.QFileDialog.getOpenFileName(None, "打开视频", "data/videos", "*.mp4;*.mkv;All Files(*)")
        except OSError as reason:
            print(str(reason))
        else:
            if not self.vid_name:
                #QtWidgets.QMessageBox.warning(self, u"Warning", u"打开视频失败", buttons=QtWidgets.QMessageBox.Ok, defaultButton=QtWidgets.QMessageBox.Ok)
                #self.ui.message_box.append("视频载入失败。")
                pass
            else:
                cap = cv2.VideoCapture(self.vid_name)

                if not cap.isOpened(): 
                    print("could not open :",self.vid_name)
                    return
                    
                length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                fps    = cap.get(cv2.CAP_PROP_FPS)
                
                print(length, width, height, fps)
                
                self.curr_fps = fps
                self.curr_maxframe = length
                self.curr_framecount = 0
                
                dir = 'detections/cache'
                if os.path.exists(dir):
                    shutil.rmtree(dir)
                    os.makedirs(dir)
                else:
                    os.makedirs(dir)
                
                self.thistime = str(time.time())
                dir = 'detections/video_cache/' + self.thistime
                if not os.path.exists(dir):
                    #shutil.rmtree(dir)
                    os.makedirs(dir)
                
                self.vid_running = True
                self.video_stream = cv2.VideoCapture(self.vid_name)
                self.timer.start(20)
                #vid = QtGui.QPixmap(self.vid_name)#.scaled(self.image_box_1.width(), self.image_box_1.height())
                #self.image_box_1.setPixmap(vid)
                
    def open_cam(self):
        self.selected = "cam"
        self.cam_running = True
        self.vid_running = False
        self.button_record.setDisabled(False)
        self.button_save.setDisabled(False)
        dir = 'detections/cache'
        if os.path.exists(dir):
            shutil.rmtree(dir)
            os.makedirs(dir)
        else:
            os.makedirs(dir)
        
        self.thistime = str(time.time())
        dir = 'detections/cam_cache/' + self.thistime
        if not os.path.exists(dir):
            #shutil.rmtree(dir)
            os.makedirs(dir)
                    
        if not self.alreadystarted_cam:
            self.alreadystarted_cam = True
            self.cap_video = cv2.VideoCapture(0)
            self.timer.start(20)
            
    def show_video(self):
        result, image = None, None
        if self.vid_running:
            result, image = self.video_stream.read()
        if result:
            im0 = image
            im0 = cv2.cvtColor(im0, cv2.COLOR_BGR2RGB)
            FlippedImage = im0
            image = QtGui.QImage(FlippedImage, FlippedImage.shape[1],FlippedImage.shape[0], FlippedImage.shape[1] * 3, QtGui.QImage.Format_RGB888)
            pix = QtGui.QPixmap(image)
            #ConvertToQtFormat = QImage(FlippedImage.data, FlippedImage.shape[1], FlippedImage.shape[0], QImage.Format_RGB888)
            #Pic = ConvertToQtFormat.scaled(640, 640, Qt.KeepAspectRatio)
            #cv2.imshow("test", FlippedImage)
            #FlippedImage = cv2.cvtColor(FlippedImage, cv2.COLOR_BGR2RGB)
            cv2.imwrite("data/images/temp_vid.jpg", FlippedImage)
            self.image_box_1.setPixmap(QtGui.QPixmap(pix))
            
            #results = self.loaded_model('data/images/temp_vid.jpg')#.save()
            #results.ims
            #results.render()
            
            #img = Image.open('data/images/temp_vid.jpg').convert('RGB')

            im0 = cv2.imread('data/images/temp_vid.jpg')
            #im0 = cv2.cvtColor(im0, cv2.COLOR_BGR2RGB)
            FlippedImage = im0
            image = QtGui.QImage(FlippedImage, FlippedImage.shape[1],FlippedImage.shape[0], FlippedImage.shape[1] * 3, QtGui.QImage.Format_RGB888)
            pix = QtGui.QPixmap(image)
            
            #ConvertToQtFormat = QImage(FlippedImage.data, FlippedImage.shape[1], FlippedImage.shape[0], QImage.Format_RGB888)
            #Pic = ConvertToQtFormat.scaled(640, 640, Qt.KeepAspectRatio)
            #cv2.imshow("test", FlippedImage)
            img = Image.open(self.img_name).convert('RGB')
            self.image_box_1.setPixmap(QtGui.QPixmap(pix))
            
            translated = inference(img, "fast", clip_model_name="ViT-B-32/openai")
            print(translated)
            res = None
            try:
                mainprompttemplate = 'When I saw ' + translated + ', I felt emotions such as'

                response = co.generate(
                  prompt= mainprompttemplate
                )
                somestr = '{}'.format(response.generations[0].text)
                input_text = mainprompttemplate + somestr
                print(input_text)
                input_text = vectorizer(input_text)
                
                res = self.loaded_model.predict(np.expand_dims(input_text, 0))
                #print(res, res[0][0])
            except Exception as e:
                input_text = vectorizer(translated)
                res = self.loaded_model.predict(np.expand_dims(input_text, 0))
                #print(res, res[0][0])
            
            im0 = cv2.imread("emotions.jpg")
            overlay = im0.copy()
            
            emotion = te.get_emotion(translated)
            
            thismodel = tf.keras.models.load_model("model/happy2.h5")
            res0 = thismodel.predict(np.expand_dims(input_text, 0))
            print(res, res[0][0])
            print(res0)
            print(emotion)
            # A filled circle
            cv2.circle(overlay, (820, 300), int(65*2*emotion["Happy"]*res0[0][0]), (20,20,20), -1)
            cv2.circle(overlay, (820, 430), int(65*2*emotion["Happy"]*res0[0][1]), (20,20,20), -1)
            cv2.circle(overlay, (820, 570), int(65*2*emotion["Happy"]*res0[0][2]), (20,20,20), -1)
            cv2.circle(overlay, (820, 720), int(65*2*emotion["Happy"]*res0[0][3]), (20,20,20), -1)
            cv2.circle(overlay, (820, 870), int(65*2*emotion["Happy"]*res0[0][4]), (20,20,20), -1)
            
            thismodel = tf.keras.models.load_model("model/sad2.h5")
            res0 = thismodel.predict(np.expand_dims(input_text, 0))
            print(res0)
            cv2.circle(overlay, (820, 1700), int(65*2*emotion["Sad"]*res0[0][0]), (20,20,20), -1)
            cv2.circle(overlay, (820, 1550), int(65*2*emotion["Sad"]*res0[0][1]), (20,20,20), -1)
            cv2.circle(overlay, (820, 1420), int(65*2*emotion["Sad"]*res0[0][2]), (20,20,20), -1)
            cv2.circle(overlay, (820, 1275), int(65*2*emotion["Sad"]*res0[0][3]), (20,20,20), -1)
            cv2.circle(overlay, (820, 1135), int(65*2*emotion["Sad"]*res0[0][4]), (20,20,20), -1)
            
            thismodel = tf.keras.models.load_model("model/anxious2.h5")
            res0 = thismodel.predict(np.expand_dims(input_text, 0))
            print(res0)
            cv2.circle(overlay, (1430, 630), int(65*2*(emotion["Surprise"]*0.25+emotion["Fear"]*0.75)*res0[0][0]), (20,20,20), -1)
            cv2.circle(overlay, (1300, 710), int(65*2*(emotion["Surprise"]*0.25+emotion["Fear"]*0.75)*res0[0][1]), (20,20,20), -1)
            cv2.circle(overlay, (1170, 790), int(65*2*(emotion["Surprise"]*0.25+emotion["Fear"]*0.75)*res0[0][2]), (20,20,20), -1)
            cv2.circle(overlay, (1050, 850), int(65*2*(emotion["Surprise"]*0.25+emotion["Fear"]*0.75)*res0[0][3]), (20,20,20), -1)
            cv2.circle(overlay, (920, 940), int(65*2*(emotion["Surprise"]*0.25+emotion["Fear"]*0.75)*res0[0][4]), (20,20,20), -1)
            
            thismodel = tf.keras.models.load_model("model/angry2.h5")
            res0 = thismodel.predict(np.expand_dims(input_text, 0))
            print(res0)
            cv2.circle(overlay, (1430, 1330), int(65*2*emotion["Angry"]*res0[0][0]), (20,20,20), -1)
            cv2.circle(overlay, (1300, 1260), int(65*2*emotion["Angry"]*res0[0][1]), (20,20,20), -1)
            cv2.circle(overlay, (1170, 1200), int(65*2*emotion["Angry"]*res0[0][2]), (20,20,20), -1)
            cv2.circle(overlay, (1050, 1130), int(65*2*emotion["Angry"]*res0[0][3]), (20,20,20), -1)
            cv2.circle(overlay, (920, 1060), int(65*2*emotion["Angry"]*res0[0][4]), (20,20,20), -1)
            
            thismodel = tf.keras.models.load_model("model/afraid2.h5")
            res0 = thismodel.predict(np.expand_dims(input_text, 0))
            print(res0)
            cv2.circle(overlay, (220, 1370), int(65*2*emotion["Fear"]*res0[0][0]), (20,20,20), -1)
            cv2.circle(overlay, (330, 1300), int(65*2*emotion["Fear"]*res0[0][1]), (20,20,20), -1)
            cv2.circle(overlay, (450, 1220), int(65*2*emotion["Fear"]*res0[0][2]), (20,20,20), -1)
            cv2.circle(overlay, (560, 1150), int(65*2*emotion["Fear"]*res0[0][3]), (20,20,20), -1)
            cv2.circle(overlay, (685, 1070), int(65*2*emotion["Fear"]*res0[0][4]), (20,20,20), -1)
            
            thismodel = tf.keras.models.load_model("model/excited2.h5")
            res0 = thismodel.predict(np.expand_dims(input_text, 0))
            print(res0)
            cv2.circle(overlay, (220, 630), int(65*2*(emotion["Surprise"]*0.3+emotion["Happy"]*0.6)*res0[0][0]), (20,20,20), -1)
            cv2.circle(overlay, (330, 710), int(65*2*(emotion["Surprise"]*0.3+emotion["Happy"]*0.6)*res0[0][1]), (20,20,20), -1)
            cv2.circle(overlay, (450, 790), int(65*2*(emotion["Surprise"]*0.3+emotion["Happy"]*0.6)*res0[0][2]), (20,20,20), -1)
            cv2.circle(overlay, (560, 850), int(65*2*(emotion["Surprise"]*0.3+emotion["Happy"]*0.6)*res0[0][3]), (20,20,20), -1)
            cv2.circle(overlay, (685, 940), int(65*2*(emotion["Surprise"]*0.3+emotion["Happy"]*0.6)*res0[0][4]), (20,20,20), -1)
            
            alpha = 0.4  # Transparency factor.

            # Following line overlays transparent rectangle
            # over the image
            image_new = cv2.addWeighted(overlay, alpha, im0, 1 - alpha, 0)
            im0 = cv2.cvtColor(image_new, cv2.COLOR_BGR2RGB)
            FlippedImage = im0
            image = QtGui.QImage(FlippedImage, FlippedImage.shape[1],FlippedImage.shape[0], FlippedImage.shape[1] * 3, QtGui.QImage.Format_RGB888)
            pix = QtGui.QPixmap(image)
            self.image_box_2.setPixmap(QtGui.QPixmap(pix))
            """
            results.ims
            results.render()
            for im in results.ims:
                #im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
                #cv2.imshow("test", im)
                FlippedImage = im
                image = QtGui.QImage(FlippedImage, FlippedImage.shape[1],FlippedImage.shape[0], FlippedImage.shape[1] * 3, QtGui.QImage.Format_RGB888)
                pix = QtGui.QPixmap(image)
                #ConvertToQtFormat = QImage(FlippedImage.data, FlippedImage.shape[1], FlippedImage.shape[0], QImage.Format_RGB888)
                #Pic = ConvertToQtFormat.scaled(640, 640, Qt.KeepAspectRatio)
                #cv2.imshow("test", FlippedImage)
                FlippedImage = cv2.cvtColor(FlippedImage, cv2.COLOR_BGR2RGB)
                cv2.imwrite("detections/images/" + str(time.time()) + ".jpg", FlippedImage)
                self.image_box_2.setPixmap(QtGui.QPixmap(pix))
            """

            if self.recording:
                FlippedImage = cv2.cvtColor(FlippedImage, cv2.COLOR_BGR2RGB)
                
                cv2.imwrite("detections/cache/" + str(time.time()) + ".jpg", FlippedImage)
                cv2.imwrite('detections/video_cache/' + self.thistime + "/" + str(time.time()) + ".jpg", FlippedImage)
            self.image_box_2.setPixmap(QtGui.QPixmap(pix))
            self.curr_framecount += 1
            if self.curr_framecount == self.curr_maxframe:
                self.vid_running = False
                #Save Video
    def show_video_cam(self):
        # reading the input using the camera
        result, image = None, None
        if self.cam_running:
            result, image = self.cap_video.read()

        # If image will detected without any error, 
        # show result
        if result:
            im0 = image
            im0 = cv2.cvtColor(im0, cv2.COLOR_BGR2RGB)
            FlippedImage = cv2.flip(im0, 1)
            image = QtGui.QImage(FlippedImage, FlippedImage.shape[1],FlippedImage.shape[0], FlippedImage.shape[1] * 3, QtGui.QImage.Format_RGB888)
            pix = QtGui.QPixmap(image)
            #ConvertToQtFormat = QImage(FlippedImage.data, FlippedImage.shape[1], FlippedImage.shape[0], QImage.Format_RGB888)
            #Pic = ConvertToQtFormat.scaled(640, 640, Qt.KeepAspectRatio)
            #cv2.imshow("test", FlippedImage)
            #FlippedImage = cv2.cvtColor(FlippedImage, cv2.COLOR_BGR2RGB)
            cv2.imwrite("data/images/temp_cam.jpg", FlippedImage)
            self.image_box_1.setPixmap(QtGui.QPixmap(pix))
            
            results = self.loaded_model('data/images/temp_cam.jpg')#.save()
            results.ims
            results.render()
            for im in results.ims:
                im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
                #cv2.imshow("test", im)
                FlippedImage = im
                image = QtGui.QImage(FlippedImage, FlippedImage.shape[1],FlippedImage.shape[0], FlippedImage.shape[1] * 3, QtGui.QImage.Format_RGB888)
                pix = QtGui.QPixmap(image)
                #ConvertToQtFormat = QImage(FlippedImage.data, FlippedImage.shape[1], FlippedImage.shape[0], QImage.Format_RGB888)
                #Pic = ConvertToQtFormat.scaled(640, 640, Qt.KeepAspectRatio)
                #cv2.imshow("test", FlippedImage)
                if self.recording:
                    FlippedImage = cv2.cvtColor(FlippedImage, cv2.COLOR_BGR2RGB)
                    
                    cv2.imwrite("detections/cache/" + str(time.time()) + ".jpg", FlippedImage)
                    cv2.imwrite('detections/cam_cache/' + self.thistime + "/" + str(time.time()) + ".jpg", FlippedImage)
                self.image_box_2.setPixmap(QtGui.QPixmap(pix))
                
    def record(self):
        _translate = QtCore.QCoreApplication.translate
        self.recording = not self.recording
        if self.recording:
            self.button_record.setText(_translate("MainWindow", "Recording..."))
        else:
            self.button_record.setText(_translate("MainWindow", "Record"))
        
    def save(self):
        if self.selected == 'video':
            dir = 'detections/videos'
            if not os.path.exists(dir):
                #shutil.rmtree(dir)
                os.makedirs(dir)
            
            try:
                dimensions = None
                for filename in glob.glob('detections/cache/*.jpg'):
                    img = cv2.imread(filename)
                    dimensions = img.shape
                    break
                print("Saving...")
                frameSize = (dimensions[1], dimensions[0])
                
                #fourcc = cv2.VideoWriter_fourcc(*'MJPG')
                out = cv2.VideoWriter('detections/videos/' + str(time.time()) + ".mp4",cv2.VideoWriter_fourcc(*'mpv4'), int(self.curr_fps), frameSize)

                for filename in glob.glob('detections/cache/*.jpg'):
                    print(filename)
                    
                    img = cv2.imread(filename)
                    #cv2.imshow('test', img)
                    out.write(img)
                print("Saved!")
                out.release()
            except Exception as e:
                print(e)
        if self.selected == 'cam':
            dir = 'detections/cams'
            if not os.path.exists(dir):
                #shutil.rmtree(dir)
                os.makedirs(dir)
            
            try:
                dimensions = None
                for filename in glob.glob('detections/cache/*.jpg'):
                    img = cv2.imread(filename)
                    dimensions = img.shape
                    break
                print("Saving...")
                frameSize = (dimensions[1], dimensions[0])
                
                #fourcc = cv2.VideoWriter_fourcc(*'MJPG')
                out = cv2.VideoWriter('detections/cams/' + str(time.time()) + ".mp4",cv2.VideoWriter_fourcc(*'mpv4'), int(self.curr_fps), frameSize)

                for filename in glob.glob('detections/cache/*.jpg'):
                    print(filename)
                    
                    img = cv2.imread(filename)
                    #cv2.imshow('test', img)
                    out.write(img)
                print("Saved!")
                out.release()
            except Exception as e:
                print(e)
        
    def load_model(self):
        try:
            self.openfile_name_model = "model/all2.h5"
        except OSError as reason:
            print(str(reason))
        else:
            if self.openfile_name_model:
                print(self.openfile_name_model)
                
                try:
                    print("Loading model")
                    self.loaded_model = tf.keras.models.load_model(self.openfile_name_model)
                    images = ["C:/Users/NakaMura/Desktop/cirno.png", "C:/Users/NakaMura/Desktop/1938640-bigthumbnail.jpg", "C:/Users/NakaMura/Desktop/unknown.png", "C:/Users/NakaMura/Desktop/image.png"]
                    #img = Image.open("C:\\Users\\NakaMura\\Desktop\\download.jpg").convert('RGB')
                    #print(inference(img, "fast", clip_model_name="ViT-B-32/openai"))
                    #result = history.score(X_test, Y_test)
                    print("Loaded Model")
                except:
                    pass
                # Images
                #img = "C:\\Users\\NakaMura\\Desktop\\Screenshot 2022-11-27 223302.jpg"  # or file, Path, PIL, OpenCV, numpy, list
                
                # Inference
                #results = model(img)
                
                # Results
                #results.save()  # or .show(), .save(), .crop(), .pandas(), etc.
                
                #QtWidgets.QMessageBox.warning(self, u"Ok!", u"loading complete！", buttons=QtWidgets.QMessageBox.Ok, defaultButton=QtWidgets.QMessageBox.Ok)
                self.output_box.append("Model loading complete!")
                self.button_load_image.setDisabled(False)
                self.button_load_video.setDisabled(False)
                self.button_load_camera.setDisabled(False)
            else:
                #QtWidgets.QMessageBox.warning(self, u"Warning", u"无权重文件，请先选择权重文件，否则会发生未知错误。", buttons=QtWidgets.QMessageBox.Ok, defaultButton=QtWidgets.QMessageBox.Ok)
                self.output_box.append("Warning!")
        # self.model_init(self,  **self.openfile_name_model )
        
    def initialize(self):
        self.init_slots()
        self.timer.timeout.connect(self.show_video_cam)
        self.timer.timeout.connect(self.show_video)
    def setupUi(self, MainWindow):
        MainWindow.setObjectName("MainWindow")
        MainWindow.resize(903, 431)
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName("centralwidget")
        self.button_load_model = QtWidgets.QPushButton(self.centralwidget)
        self.button_load_model.setGeometry(QtCore.QRect(10, 50, 131, 31))
        self.button_load_model.setObjectName("button_load_model")
        self.button_load_image = QtWidgets.QPushButton(self.centralwidget)
        self.button_load_image.setGeometry(QtCore.QRect(10, 90, 131, 31))
        self.button_load_image.setObjectName("button_load_image")
        self.button_load_video = QtWidgets.QPushButton(self.centralwidget)
        self.button_load_video.setGeometry(QtCore.QRect(10, 130, 131, 31))
        self.button_load_video.setObjectName("button_load_video")
        self.button_load_camera = QtWidgets.QPushButton(self.centralwidget)
        self.button_load_camera.setGeometry(QtCore.QRect(10, 170, 131, 31))
        self.button_load_camera.setObjectName("button_load_camera")
        self.button_record = QtWidgets.QPushButton(self.centralwidget)
        self.button_record.setGeometry(QtCore.QRect(10, 210, 131, 31))
        self.button_record.setObjectName("button_record")
        self.output_box = QtWidgets.QTextEdit(self.centralwidget)
        self.output_box.setGeometry(QtCore.QRect(10, 290, 131, 121))
        self.output_box.setObjectName("output_box")
        self.image_box_1 = ScaledPixmapLabel(MainWindow)
        self.image_box_1.setGeometry(QtCore.QRect(160, 50, 361, 361))
        self.image_box_1.setFrameShape(QtWidgets.QFrame.Box)
        self.image_box_1.setText("")
        self.image_box_1.setObjectName("image_box_1")
        self.image_box_1.setScaledContents(True)
        self.image_box_2 = ScaledPixmapLabel(MainWindow)
        self.image_box_2.setGeometry(QtCore.QRect(530, 50, 361, 361))
        self.image_box_2.setFrameShape(QtWidgets.QFrame.Box)
        self.image_box_2.setText("")
        self.image_box_2.setObjectName("image_box_2")
        self.image_box_2.setScaledContents(True)
        self.image_label_1 = QtWidgets.QLabel(self.centralwidget)
        self.image_label_1.setGeometry(QtCore.QRect(260, 20, 131, 21))
        self.image_label_1.setAlignment(QtCore.Qt.AlignCenter)
        self.image_label_1.setObjectName("image_label_1")
        self.image_label_2 = QtWidgets.QLabel(self.centralwidget)
        self.image_label_2.setGeometry(QtCore.QRect(65*20, 20, 131, 21))
        self.image_label_2.setAlignment(QtCore.Qt.AlignCenter)
        self.image_label_2.setObjectName("image_label_2")
        self.button_save = QtWidgets.QPushButton(self.centralwidget)
        self.button_save.setGeometry(QtCore.QRect(10, 250, 131, 31))
        self.button_save.setObjectName("button_save")
        MainWindow.setCentralWidget(self.centralwidget)
        
        self.initialize()
        
        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

    def retranslateUi(self, MainWindow):
        _translate = QtCore.QCoreApplication.translate
        MainWindow.setWindowTitle(_translate("MainWindow", "MainWindow"))
        self.button_load_model.setText(_translate("MainWindow", "Load Model"))
        self.button_load_image.setText(_translate("MainWindow", "Load Image"))
        self.button_load_video.setText(_translate("MainWindow", "Load Video"))
        self.button_load_camera.setText(_translate("MainWindow", "Load Camera"))
        self.button_record.setText(_translate("MainWindow", "Record"))
        self.image_label_1.setText(_translate("MainWindow", "Image 1"))
        self.image_label_2.setText(_translate("MainWindow", "Image 2"))
        self.button_save.setText(_translate("MainWindow", "Save"))
        

if __name__ == "__main__":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = Ui_MainWindow()
    ui.setupUi(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())
